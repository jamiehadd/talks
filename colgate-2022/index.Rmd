---
title: "Generative Hypergraph Clustering: <br> Scalable Heuristics and Sparse Thresholds"                 
subtitle: "Department of Computer Science <br> Colgate University<br> February 4th, 2022"
author: "Dr. <span class = 'author-highlight'>Phil Chodrow</span> <br> Department of Mathematics <br> University of California, Los Angeles "           
date: ""              
output:                         
  xaringan::moon_reader:         
    lib_dir: libs             
    css: ["../assets/ninpo.css",  "../assets/ninjutsu.css",   "../assets/shinobi.css", "../assets/pc_custom.css", "css/pc_custom.css"]    
    seal: true   
    self_contained: false  
    nature: 
      ratio: "16:10"
      highlightStyle: github     
      highlightLines: true 
      countIncrementalSlides: false  
---
exclude: true   
<style type="text/css">
code.r{ 
  font-size: 16px; 
}
pre {
  font-size: 16px !important;  
}
</style>
```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r run, eval=FALSE, include=FALSE}
# xaringan::inf_mr("colgate-2022/index.rmd")
``` 


```{r child = '../slides-shared/geo-intro.Rmd'}
```

---
```{r child = '../slides-shared/my-research.Rmd'}
```

---

class: split-40
layout: true

.column.bg-main1[
### Foundations of Network Data Science

<br>

What .alert[models] accurately reflect features of network data? 

What .alert[algorithms] can we use to learn these models? 

What can we learn about the .alert[properties] about these models and algorithms?

]

.column[.split-four[
  .row[.content[.split-20[.column[ 
  <div class="thumbnail"> 
  <img src="../img-shared/heatmap-thumbnail.png" width=130px> 
  </div>
  ].column[
  <div class="compress">
  <br>
  <a href="https://link.springer.com/article/10.1007/s41109-020-0252-y">Nonbacktracking specral clustering of nonuniform hypergraphs </a> <br>
  <b>PSC</b>, Nicole Eikmeier, and Jamie Haddock<br>
  <i>In preparation</i> (2022)
  </div>       
  ]]] 
]
  .row[.content[.split-20[.column[
    <div class="thumbnail"> 
    <img src="../img-shared/contact-clustering-excerpt.png" width=110px> 
    </div>
    ].column[
    <div class="compress">
    <br>
    <a href="https://advances.sciencemag.org/content/7/28/eabh1303">Generative hypergraph clustering: from blockmodels to modularity </a> <br>
    <b>PSC</b>, Nate Veldt, and Austin Benson <br>
    <i>Science Advances</i> (2021)
    </div>       
    ]]
  ]]
.row[.content[.split-20[.column[
  <div class="thumbnail"> 
    <img src="https://www.philchodrow.com/assets/img/thumbnails/error_matrices.png" width=120px> 
    </div>
    ].column[
    <div class="compress">
    <br>
    <a href="https://epubs.siam.org/doi/abs/10.1137/19M1288772">Moments of uniformly random multigraphs with fixed degree sequences </a> <br>
    <b>PSC</b><br>
    <i>SIAM J. Mathematics of Data Science</i> (2020)
    </div>       
    ]]]
  ]
.row[.content[.split-20[.column[.content[
  <div class="thumbnail"> 
  <img src="https://www.philchodrow.com/assets/img/thumbnails/configuration-hypergraphs.png" width=120px> 
  </div>
  ]].column[.content[
  <div class="compress">
  <br>
  <a href="https://academic.oup.com/comnet/article-abstract/8/3/cnaa018/5879929">Configuration models of random hypergraphs </a> <br>
  <b>PSC</b> <br>
  <i>J. Complex Networks</i> (2020)
  </div>       
  ]]]]
]
]
]

???

That said, I want to focus on a specific story on the foundations of network data science. This story has some nice mathematical features; I feel that it represents my style well; and it highlights the interplay between data science, computation, and dynamics that animates a lot of my work. 

---


---

class: split-two
layout: false

.column.bg-main1[
.content[ 
  ## Graphs and Hypergraphs 
 
  <br> 
  A .alert[graph] consists of a set of nodes $\mathcal{N}$ and a set of edges $\mathcal{E}$. Each edge in $\mathcal{E}$ is a set of two nodes. 

<br> <br> <br> <br> <br>
  In .alert[hypergraphs], edges in $\mathcal{E}$ can contain *any number* of nodes.     
]

]

.column[
  .vmiddle[
  <img src="../img-shared/graph-hypergraph.png" width=100%></img>
  ] 
]

---

class: split-two
layout: false

.column.bg-main1[
  
  ## Hypergraph Data

  

]

.column.bg-main4[.vmiddle[
  <br>
]]


---

class: split-two
layout: false

.column.bg-main1[
  
  ## Hypergraph Data

<br> <br> 

  - .alert[**Interaction**]: nodes are agents, edges are interaction events (socializing in groups, attending events).
  

]

.column.bg-main4[.vmiddle[
  <img src="../img-shared/social-interaction.jpeg" width=100%></img> 
]]

---

class: split-two
layout: false

.column.bg-main1[
  
  ## Hypergraph Data

<br> <br> 

  - .alert[**Interaction**]: nodes are agents, edges are interaction events (socializing in groups, attending events).
  - .alert[**Collaboration**]: nodes are collaborators, edges are projects or teams (scholarly papers, legislation, etc). 
  

]

.column.bg-main4[.vmiddle[
  <img src="../img-shared/teamwork.jpeg" width=100%></img> 
]]

---

class: split-two
layout: false

.column.bg-main1[
  
  ## Hypergraph Data

<br> <br> 

  - .alert[**Interaction**]: nodes are agents, edges are interaction events (socializing in groups, attending events).
  - .alert[**Collaboration**]: nodes are collaborators, edges are projects or teams (scholarly papers, legislation, etc). 
  - .alert[**Co-presence**]: nodes are chemical compounds, edges are drugs formed from those compounds.

]

.column.bg-main4[.vmiddle[
  <img src="../img-shared/mccoy.png" width=100%></img>  
]]

---




layout: false
class: split-two

.column.bg-main1[
  ### The Hypergraph Community Detection Problem 

  Given some hypergraph data, assign each node to a .alert[*community*] (or *cluster*) of "related" nodes. 
  <br> <br>
  "*Related*": often interpreted as "*densely interconnected.*"
<br> <br>
  Applications in social network analysis, drug discovery, image processing, data visualization...

  
  .footnote[
  One review in: <br> <b>PSC</b>, N. Veldt, A. R. Benson (2021). Generative hypergraph clustering: from blockmodels to modularity, <i>Science Advances</i>, 7:eabh1303
]

  
]
.column[.content.vmiddle[.stretch[
  <img src="../img-shared/detection-1.png" width=100%>
]]]


---


layout: true
class: split-two middle 
 
.column[
  .split-three[ 
  .row.bg-main1[.content.vmiddle[.font_medium[Generative clustering with .alert[hypergraph blockmodels]].
  ]]    
  .row.bg-main2[.content.vmiddle[.font_medium[Scalable, greedy heuristics for .alert[large hypergraphs].] 
  ]] 
  .row.bg-main3[.content.vmiddle[.font_medium[.alert[Sparse thresholds] and eigenvector methods.] 
  ]]
]
] 

.column[.center[.stretch[
  {{content}} 
]]]

---
class: hide-row2-col1 hide-row3-col1 hide-row4-col1 hide-row5-col1 

<br>
<img src="../img-shared/pure-fiction.png" width=100%>

---
class: hide-row3-col1 hide-row4-col1 hide-row5-col1


<img src="../img-shared/contact-clustering-excerpt.png" width=100%>

  
---
class: hide-row4-col1   hide-row5-col1 

<img src="../img-shared/spectrum.png" width=75%> 
<img src="../img-shared/heatmap-exp-1.png" width=85%> 

---

class: fade-row2-col1 fade-row3-col1 fade-row4-col1 fade-row5-col1

  <br>
  <img src="../img-shared/pure-fiction.png" width=100%>

---

layout: false
background-image: url(../img-shared/task.png)
background-size: contain

---

layout: false
background-image: url(../img-shared/fiction.png)
background-size: contain


---

class: split-two
layout: false

.column[ 
  ### Hypergraph Stochastic Blockmodel
  <img src="../img-shared/pure-fiction.png" width=90%>
]

.column.bg-main1[


.font_smaller[

### &nbsp; <br>  &nbsp;  
1. Give each node $i$ a cluster label $z_i$ and a degree-weight $\theta_i$ (higher weight $\implies$ more hyperedges involving that node).
2. For each tuple $R$ of nodes, sample a random number of edges:
$$a_R \sim \mathrm{Poisson}\left(\Omega(\mathbf{z}_R)\prod_{i \in R}\theta_i\right)$$

Here, $\Omega$ is an *affinity function* that encodes which combinations of groups are likely to form hyperedges. 
]

.footnote[
Bernoulli variant proposed by Ke et al (2019), 	
Community detection for hypergraph networks via regularized tensor power iteration, *arXiv:*:1909.06503
]
]

---

class: split-two
layout: false

.column[ 
  ### Hypergraph Stochastic Blockmodel
  <img src="../img-shared/pure-fiction.png" width=90%>
]

.column.bg-main1[

<br> <br> <br> 
## Today's Big Message

The generative approach to hypergraph clustering offers both .alert[scalable heuristics] and .alert2[fundamental theoretical insights]. 
]

---

layout: true
class: split-two middle 
 
.column[
  .split-three[ 
  .row.bg-main1[.content.vmiddle[.font_medium[Generative clustering with .alert[hypergraph blockmodels]].
  ]]    
  .row.bg-main2[.content.vmiddle[.font_medium[Scalable, greedy heuristics for .alert[large hypergraphs].] 
  ]] 
  .row.bg-main3[.content.vmiddle[.font_medium[.alert[Sparse thresholds] and eigenvector methods.] 
  ]]
]
] 

.column[.center[.stretch[
  {{content}} 
]]]

---

class: fade-row1-col1 fade-row3-col1 fade-row4-col1 fade-row5-col1

  <img src="../img-shared/contact-clustering-excerpt.png" width=100%>

---

class: split-two 
layout: false

.column.bg-main1[

<br> <br>
### Motivation 

Suppose we have some hypergraph Big Data® with suspected cluster structure.   

### Question

Can we do .alert[scalable] clustering with statistical foundations?


]

.column[.stretch[ 
  <img src="../img-shared/hmod-paper.png" width=100%>   
]]

---
class: split-50 bg-main1 
layout: false 
 
.row[ 
.split-three[
.column[<br><br>
  <img src="../img-shared/nate_portrait.jpeg" width=90%> 
  ]
.column[<br><br>
  <img src="../img-shared/austin.jpg" width=90%> 
]
.column[<br><br>
  <img src="../img-shared/phil_portrait.jpeg" width=90%>   
] 

]
]
.row[ 
.split-three[
.column[<br>
  .font_large[.alert-no-bold[<nobr>Nate Veldt</nobr>]]
  <nobr>Computer Science</nobr> <br> Texas A&M      
  .alert2[@n_veldt]
]  
.column[<br>
  .font_large[.alert-no-bold[<nobr>Austin Benson</nobr>]]
   Computer Science <br> Cornell      
   .alert2[@austinbenson]
]
.column[<br>
  .font_large[.alert-no-bold[<nobr>Phil Chodrow</nobr>]]
  <nobr>Mathematics</nobr> <br> UCLA
  <br> .alert2[@PhilChodrow] <br>
]
]
]



---

class: split-two
layout: 

.column.bg-main1[
### The Affinity Function $\Omega$

<br> 

$$a_R \sim \mathrm{Poisson}\left(\Omega(\mathbf{z}_R)\prod_{i \in R}\theta_i\right)$$

$\Omega$ controls how hyperedges form in response to node labels. 

Let's focus on the .alert[All-Or-Nothing] affinity.

This models the idea that edges form most frequently when nodes have the same labels. 


]

.column[

### <br>

<br> <br>

Let $e$ be a hyperedge of size $k$: 

$$\Omega(\mathbf{z}_e) = \begin{cases}
  \omega_{k1} &\quad e \text{ has all same labels} \\ 
  \omega_{k0} &\quad \text{otherwise}\;.  
\end{cases}$$

<br>

$$\Omega(\color{#ff5252}{\bullet} \color{#ff5252}{\bullet} \color{#ff5252}{\bullet}) > \Omega(\color{#ff5252}{\bullet} \color{#ff5252}{\bullet} \color{#FFD046}{\bullet})= \Omega(\color{#ff5252}{\bullet} \color{#FFD046}{\bullet} \color{#6ec0de}{\bullet})\ldots$$ 

]

<!-- ---

class: split-two
layout: false


.column.bg-main1[
  ## Maximum-Likelihood Inference

  <br>

  We only observe $H$. How do we learn the clusters $\mathbf{z}$?

  Let $P(H; \theta, \mathbf{z}, \omega)$ be the probability of realizing a hypergraph $H$ with parameters $(\theta, \mathbf{z}, \omega)$. 

  .alert[Maximum-Likelihood Inference]: 

  $$(\hat{\theta}, \hat{\mathbf{z}}, \hat{\omega}) = \text{argmax}_{\theta, \mathbf{z}, \omega} \log P(H; \theta, \mathbf{z}, \omega).$$ 
]

.column[
  
  <br> <br> <br>

  Some estimates can be approximated quickly:

<br> 
  **Degree Weights**

  $$\hat{\theta}_i \approx\text{degree of node }i \equiv d_i$$

  **Affinity Values** (requires estimate of $\mathbf{z}$)

.font_smaller[
  $$\omega_{k1} \approx \frac{\text{# homogeneous hyperedges }}{\prod_{j} (\text{sum of degrees in cluster } j)}\;.$$
]

.footnote[Approximations exact when all clusters are same size]

] -->


---

## Estimating labels $\mathbf{z}$

With the All-Or-Nothing affinity, we can derive an objective function from a maximum-likelihood problem for our blockmodel: 

$$
Q \triangleq \sum_k \color{#ff5252}{\beta_k} \left[(\text{# homogeneous }k\text{-edges}) - \color{#21728f}{\gamma_k}\sum_j (\text{# of edges in cluster } j)^k\right]
$$

$\color{#ff5252}{\beta_k} \triangleq \log \omega_{k1} - \log \omega_{k0}$. <br>
$\color{#21728f}{\gamma_k} \triangleq \frac{1}{\beta_k}(\omega_{k1} - \omega_{k0})$.

**Maximum-Likelihood Problem**: find cluster labels $\mathbf{z}$ to make $Q$ large. 

 
.footnote[
   
Derivation follows Newman, "Equivalence between modularity optimization and maximum likelihood methods for community detection." *PRE*, 2016
  
Generalizes Kamiński et al., "Clustering via hypergraph modularity." *PLoS ONE*, 2019] 

---

class: split-two

.column.bg-main1[

### Hypergraph Maximum-Likelihood Louvain Algorithm

<br>

1. All nodes start in their own cluster. 
2. Greedily .alert[merge] nodes to maximize $Q$. 
3. Then, greedily merge .alert2[entire clusters] of nodes.  
4. Estimate parameters $\omega$ and $\{\theta_i\}$.
5. Repeat!

- Extends Louvain algorithm for graphs.
- Lots of small tricks to make this fast. 



.footnote[Blondel et al. "Fast unfolding of communities in large networks." *J. Stat. Mech.*, 2008]
]

.column[ 
  
  <br>
  <img src="../img-shared/louvain.png" width=100%>
  
  .footnote[Image from Traag et al., "From Louvain to Leiden: guaranteeing well-connected communities," *Scientific Reports*, 2019]
]

---
background-image: url(../img-shared/performance.png)
background-size: contain

#### We can retrieve correlated partitions on synthetic hypergraphs of .alert[1M nodes].

<br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> 

Hypergraph Louvain is roughly as fast as baseline methods based on graph projections. 

---

### .alert[Empirical data]: school contact hypergraphs. 

<br> 
.center[<img src="../img-shared/sociopatterns.jpg" width=70%>]

Cluster labels are classes in a primary school and a high school (in France). 


.footnote[
Stehlé et al. (*PLoS ONE*, 2011) and Mastrandrea et al. (*PLoS ONE*, 2015)
]

---


background-image: url(../img-shared/contact-clustering.png)
background-size: contain

---

background-image: url(../img-shared/recovery_experiments.png)
background-size: contain

---

class: split-two

.column.bg-main1[
  
  ## Summing Up

  <br>

  The generative approach informs .alert[scalable algorithms] for clustering hypergraphs. 

  These algorithms .alert[inherit assumptions] from their generative models. 

  These algorithms can .alert[outperform graph-based methods] on social interaction data sets. 

]

.column[.stretch[ 
  <img src="../img-shared/hmod-paper.png" width=100%>   
]]

---

layout: true
class: split-two middle 
 
.column[
  .split-three[ 
  .row.bg-main1[.content.vmiddle[.font_medium[Generative clustering with .alert[hypergraph blockmodels]].
  ]]    
  .row.bg-main2[.content.vmiddle[.font_medium[Scalable, greedy heuristics for .alert[large hypergraphs].] 
  ]] 
  .row.bg-main3[.content.vmiddle[.font_medium[.alert[Sparse thresholds] and eigenvector methods.] 
  ]]
]
] 

.column[.center[.stretch[
  {{content}} 
]]]

---

class: fade-row1-col1 fade-row2-col1 fade-row4-col1 fade-row5-col1

<img src="../img-shared/spectrum.png" width=75%> 
<img src="../img-shared/heatmap-exp-1.png" width=85%> 

---

layout: false
class: split-two

.column.bg-main1[ 
  ### Sparsity

  <br>
  A hypergraph blockmodel is .alert[*sparse*] if $\Omega$ is tuned so that 

  $$ \Omega(\mathbf{z}_R) = O(n^{1-\left|{R}\right|})\;. $$

  **Consequence**: \# edges attached to a node doesn't grow with the size of the hypergraph. 

  .alert[Intuition]: if the world's population doubled, the number of friends that you personally have might not change too much. 
]

.column[
  ### Fundamental Limits

  <br>
  In graphs, there are sparse blockmodels for which algorithms can't even .alert[sometimes] find clusters that are even .alert2[correlated] with the correct ones.   

  
  <br>
  What about hypergraphs?

  .footnote[
    Decelle et al. (2011) Inference and phase transitions in the detection of modules in sparse networks, *Phys. Rev. Let.* 107.6: 065701
    <br> <br>
    Mossel et al. (2018) A proof of the blockmodel threshold conjecture, *Combinatorica*.  
  ]
  
]

---
class: split-50 bg-main1 
layout: false 
 
.row[ 
.split-three[
.column[<br><br>
  <img src="../img-shared/eikmeier-3.png" width=90%> 
]
.column[<br><br>
  <img src="../img-shared/jamie_portrait.jpeg" width=90%> 
  ]
.column[<br><br>
  <img src="../img-shared/phil_portrait.jpeg" width=90%>   
] 

]
]
.row[ 
.split-three[
.column[<br>
  .font_large[.alert-no-bold[<nobr>Nicole Eikmeier</nobr>]]
   Computer Science <br> Grinnell College      
   .alert2[@NicoleEikmeier]
]
.column[<br>
  .font_large[.alert-no-bold[<nobr>Jamie Haddock</nobr>]]
  <nobr>Mathematics</nobr> <br> Harvey Mudd College      
  .alert2[@jamie_hadd]
]  
.column[<br>
  .font_large[.alert-no-bold[<nobr>Phil Chodrow</nobr>]]
  <nobr>Mathematics</nobr> <br> UCLA
  <br> .alert2[@PhilChodrow] <br>
]
]
]

---

class: 

### A Testbed for Sparse Hypergraph Clustering

<img src="../img-shared/testbed-narrow.png" width=100%>


---

class: split-two
layout: true

.column[

## Synthetic Testbed

<br> <br>
<img src="../img-shared/testbed-narrow.png" width=100%>   
]

.column[.stretch[ 
  {{content}}
]]


---

<img src="../img-shared/hmod-paper.png" width=100%>   

---

## From the Prequel

<br>
<img src="../img-shared/louvain-detection.png" width=80%>   

Adjusted Rand Index (ARI): 

- ARI = 1: perfect community detection. 
- ARI = 0: random noise.

---

layout: false
class: split-two

.column.bg-main1[

### The Belief-Propagation Jacobian

<br> 
.alert[Belief-propagation] is a method for learning probabilistic machine learning models, but it's very expensive on hypergraphs. 

<br>

We can approximate BP by computing eigenvectors of the .alert[Jacobian matrix], evaluated at an uninformative fixed point.  

Eigenvectors of the Jacobian (sometimes) give useful cluster information. 

]
.column[
  

### &nbsp; 

<br> 
**Theorem (PSC, JH, NE '22)**: In a sparse HSBM, 

  $$\mathcal{J} = \sum_{k = 1}^{\bar{k}} \mathbf{C}_k \otimes \mathbf{B}_k + O(n^{-1})\;,$$

  - $\mathbf{C}_k$ is a matrix of parameters that depends on the affinity function $\Omega$. 
  - $\mathbf{B}_k$ is a matrix called the *Hashimoto operator* for edges of size $k$. 
  - $\otimes$ is the matrix Kronecker product. 


.footnote[
  .alert2[Shown for graphs by]: <br> Krzakala et al. (2013)  Spectral redemption in clustering sparse networks, <i>PNAS</i> 110 (52) 20935-20940
]
]

<!-- ---
layout: false
class: split-two

.column.bg-main1[
## The Hashimoto Operator

<br>
The .alert[Hashimoto operator] $\color{#FFD046}{\mathbf{B}_k}$ is a matrix that operates on *edge-node pairs*. 

Define $(e_1, p_1) \rightarrow_k (e_2, p_2)$: 

- $p_1 \in e_1$ and $p_2 \in e_2$
- $p_1 \in e_2 \setminus p_2$
- $e_1 \neq e_2$
- $\lvert e_2\rvert = k$. 

Then, 

.font_smaller[ .font_smaller[
$$\mathbf{B}_k[(e_1, p_1), (e_2, p_2)] = \begin{cases} 1 &\quad (e_1, p_1) \rightarrow_k (e_2, p_2) \\ 
0 &\quad \text{otherwise.}\end{cases}$$
]]]

.column[
<br>
<img src="../img-shared/hypergraph-nonbacktracking.png" width=100%>

"I can get to $p_2 \in e_2$ from $e_1$ by passing through $p_1$. I can get to $p_3 \in e_3$ from $e_2$ by passing through $p_2$..."

]

---
class: split-two
layout: false


.column.bg-main1[
## The Hashimoto Operator

<br> 
Popularized (for graphs) by Hashimoto, K. (1990), *Int. J. Math.* 

Important theorem for computation by Bass, H. (1992), *Int. J. Math.* 

Formulated for hypergraphs by Storm, C. K. (2006). *The Electronic Journal of Combinatorics*.

"Rediscovered" for hypergraphs by Angelini, M. C. et al. (2015), *Allerton Conference*.

]

.column[
<br>
<img src="../img-shared/hypergraph-nonbacktracking.png" width=100%>

"I can get to $p_2 \in e_2$ from $e_1$ by passing through $p_1$. I can get to $p_3 \in e_3$ from $e_2$ by passing through $p_2$..."
] -->

---

class: split-two

## An Ihara-Bass Theorem for the BP Jacobian

$\mathbf{J} = \sum_k \mathbf{C}_k\otimes\mathbf{B}_k \approx \mathcal{J}$ can be a very large matrix $\implies$ slow eigenvalue computations. 

**Theorem (PSC, JH, NE '22)**: Under mild conditions, if $\lambda$ is an "interesting" eigenvalue of $\mathbf{J},$ then $\lambda$ is also an eigenvalue of the $2n\ell\bar{k} \times 2n\ell\bar{k}$ matrix

.font_smaller[
$$\mathbf{J}' = (\mathbf{G}\otimes \mathbf{I}_n) \left[\begin{matrix} 
\mathbf{0} & \mathbf{I}_\ell \otimes \mathbb{D} \\ 
\mathbf{0} & \mathbf{I}_\ell \otimes \mathbb{A}
\end{matrix}\right] - \bar{\mathbf{G}} \left[\begin{matrix} 
\mathbf{0} & \mathbf{I}_\ell \otimes \mathbf{I}_{\bar{k}} \\ 
\mathbf{I}_\ell \otimes (\mathbf{K} - \mathbf{I}_{\bar{k}-1}) & \mathbf{I}_\ell \otimes (\mathbf{K} - 2\mathbf{I}_{\bar{k}-1})
\end{matrix}\right] \otimes \mathbf{I}_n$$
]

- $\ell$ is the number of clusters and $\bar{k}$ the number of distinct edge sizes. 
- $\mathbf{G}$, $\bar{\mathbf{G}}$ hold statistical parameters
- $\mathbb{D}$ is a degree operator,  $\mathbb{A}$ is an adjacency operator,  and $\mathbf{K}$ is a diagonal matrix containing the possible edge sizes.   

.footnote[
  Similar in spirit to a famous result by Bass (1992, *Int. J. Math*).  
]

<!-- ---



class: split-two

.column.bg-main1[

### Belief-Propagation Spectral Clustering

<br> 

1. Start with a guess about $\mathbf{C}_k$ and form $\mathbf{J}'$. 
2. Compute the .alert[leading eigenvectors] of $\mathbf{J}'$ with real eigenvalues. 
3. For each eigenvector $\mathbf{v} = (\alpha, \beta)$, compute $u_{i\ell} = \mathbb{1}\left(\sum_{k = 1}^{\bar{k}} \alpha_{ik}^{(\ell)} > 0\right)$. 
4. Use a .alert2[Euclidean clustering algorithm] (like $k$-means) in the space spanned by the vectors $\{\mathbf{u}\}$. 
5. Re-estimate $\mathbf{C}_k$ and repeat...

]



.column[.vmiddle[.center[

<img src="../img-shared/PCA-synthetic.png" width=90%> 

]]]



 -->


---

background-image: url(../img-shared/algorithm-demo.png)
background-size: contain

### Belief-Propagation Spectral Clustering

---

class: split-two
layout: true

.column[

## Synthetic Testbed

<br> <br>
<img src="../img-shared/testbed-narrow.png" width=100%>   
]

.column[.stretch[ 
  {{content}}
]]

---

class: 

## From the Prequel

<br>
<img src="../img-shared/louvain-detection.png" width=80%>   

Adjusted Rand Index (ARI): 

- ARI = 1: perfect community detection. 
- ARI = 0: random noise.

???

Recall that we wanted to *fill in the gaps*. 

---

class: 

## Eigenvector Algorithm

<br>
<img src="../img-shared/heatmap-exp-1-no-curve.png" width=80%>

Adjusted Rand Index (ARI): 

- ARI = 1: perfect community detection. 
- ARI = 0: random noise.

???

Oops! Uh, new gaps. Time to take a detour for some more advanced machinery. 

---

class: 

## Eigenvector Algorithm


<br>
<img src="../img-shared/heatmap-exp-1.png" width=80%>

Adjusted Rand Index (ARI): 

- ARI = 1: perfect community detection. 
- ARI = 0: random noise.


---

layout: false
class: split-two

.column.bg-main1[
### Detectability (Algorithmic)

.font_smaller[
**Conjecture**: In a 2-group blockmodel with edge sizes $k_1,k_2,\ldots$ and $c_k$ edges of size $k$ per node, .alert[spectral clustering] fails to detect clusters in the ellipsoid with centroid $(x_{k_1},x_{k_2},\ldots)$ and radii $(r_{k_1},r_{k_1}\ldots)$, where: 


$$
\begin{align}
x_k &= \frac{1-a_k}{2-a_k} d \\ 
r_k &= \frac{\sqrt{(k-1)c_k}}{2-a_k} \\
a_k &= \frac{1-2^{2-k}}{1-2^{1-k}}\;.
\end{align}
$$
]

.footnote[Proof requires controlling spectrum of Hashimoto operator, possibly generalizing approach used in: 

Bordenave et al. (2018): Non-backtracking spectrum of random graphs: community detection and non-regular Ramanujan graphs. *Annals of Probability*.
]

]

.column[.stretch[
.center[ <br> <br> <br>
<img src="../img-shared/4-heatmaps-2.png" width=100%>
]
]
]

---

class: split-two

.column.bg-main1[
### Detectability (Fundamental)

.font_smaller[
**Conjecture**: In a 2-group blockmodel with edge sizes $k_1,k_2,\ldots$ and $c_k$ edges of size $k$ per node, .alert[any algorithm] fails to detect clusters in the ellipsoid with centroid $(x_{k_1},x_{k_2},\ldots)$ and radii $(r_{k_1},r_{k_1}\ldots)$, where: 


$$
\begin{align}
x_k &= \frac{1-a_k}{2-a_k} d \\ 
r_k &= \frac{\sqrt{(k-1)c_k}}{2-a_k} \\
a_k &= \frac{1-2^{2-k}}{1-2^{1-k}}\;.
\end{align}
$$
]

.footnote[Generalizes recent theorem for graph blockmodels: 

Mossel et al. (2018) A proof of the blockmodel threshold conjecture, *Combinatorica*.  
]

]

.column[.stretch[
.center[ <br> <br> <br>
<img src="../img-shared/4-heatmaps-2.png" width=100%>
]
]
]


---

class: split-two
layout: false

.column.bg-main1[

## Summing Up

<br>

Belief-propagation spectral clustering can help us form conjectures about when detecting clusters is .alert[even possible].  

Proving these conjectures is likely to require some powerful machinery from random matrix theory and discrete probability. 

]

.column[.stretch[
.center[ <br> <br> <br>
<img src="../img-shared/4-heatmaps-2.png" width=100%>
]
]
]

---

class: bg-main1

# Today's Big Message

<br> <br>

.font_large[
The generative approach to hypergraph clustering offers both .alert[scalable heuristics] and .alert2[fundamental theoretical insights]. ]

---

```{r child = '../slides-shared/future-work-cs.Rmd'}
```

---

class: split-two

.column.bg-main1[
# **Thanks!**

.row[ 
.split-two[
.column[.lil-stretch[<br><br><br><br> 
  <img src="../img-shared/nate.jpg" width=80%> 
  .alert[Nate Veldt] <br> Texas A&M 
  ]
  ]
.column[
  .lil-stretch[<br><br><br><br>
  <img src="../img-shared/austin.jpg" width=80%> 
  .alert[Austin Benson] <br> Cornell 
  ]
]]]

<br> <br> <br> <br> <br> <br> <br> <br> <br>
<b>PSC</b>, N. Veldt, A. R. Benson, (2021). Generative hypergraph clustering: from blockmodels to modularity, <i>Science Advances</i>, 7:eabh1303
]

.column[ 
# &nbsp;   

.row[ 
.split-two[
.column[.lil-stretch[<br><br><br><br> 
  <img src="../img-shared/eikmeier-3.png" width=80%> 
  .alert[**Nicole Eikmeier**] <br> Grinnell 
  ]
  ]
.column[
  .lil-stretch[<br><br><br><br>
  <img src="../img-shared/jamie_portrait.jpeg" width=80%> 
  .alert[**Jamie Haddock**] <br> Harvey Mudd 
  ]
]]]

<br><br><br><br><br><br><br><br><br>
<b>PSC</b>, N. Eikmeier, J. Haddock, (2022). Nonbacktracking spectral clustering of nonuniform hypergraphs, <i>In preparation</i>
]

---

class: middle bg-main4  

# Extra slides


---

## High School Social Contacts

<br> 

  $n = 327$ students (nodes) in a French high school. 

  $m = 7,818$ social contact events (edges) measured by wearable 
  sensors.   

  Average number of participants in interaction $\langle k \rangle = 2.3$ 

  Cluster labels are the classes to which students are assigned. 




  <div class="footnote">
    Data originally from: <br>
    R. Mastrandrea et al. (2015), Contact patterns in a high school: A comparison between data collected using wearable sensors, contact diaries, and friendship surveys. <i>PLoS One</i> 10:9, e0136497
    <br> <br> 
    Prepared by 
    A. R. Benson et al. (2018), Simplicial closure and higher-order link prediction. <i>Proceedings of the National Academy of Sciences</i> 10.1073/pnas.1800683115  
  </div>

---
background-image: url(../img-shared/contact-high-school-classes.png)
background-size: contain

## High School Social Contacts 

---

  ## On the Other Hand...Senate Bills

<br> 

  $n = 293$ U.S. senators (nodes) cosponsoring bills.

  $m = 20,006$ bills (edges) in period 1973-2016.

  Average number of cosponsors $\langle k \rangle = 7.3$.

  Community labels are Democrat/Republican. 


<div class="footnote">
  Data originally from: <br>
  J. Fowler (2006), Legislative cosponsorship networks in the U.S. House and Senate. <i>Social Networks</i> 28:4, 454--465
  <br> <br> 
  Prepared by 
  A. R. Benson et al. (2018), Simplicial closure and higher-order link prediction. <i>Proceedings of the National Academy of Sciences</i> 10.1073/pnas.1800683115  
</div>


---
background-image: url(../img-shared/SN-congress-bills.png)
background-size: contain

## Senate Bills

---
class: bg-main2

<br> <br> <br> <br>  
### .alert[Big Picture]: you want hypergraph methods when edges of different sizes give you different information about the community structure. 

---

class: middle 

# My Papers

---

```{r child = '../slides-shared/my-papers-cs.Rmd'}
```