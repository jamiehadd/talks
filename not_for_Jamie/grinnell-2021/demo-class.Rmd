---
title: "Optimization Algorithms <br> Analysis, Applications, and Ethics"                 
subtitle: "CSC 301 <br> Grinnell College"
author: "Dr. <span class = 'author-highlight'>Phil Chodrow</span> <br> University of California, Los Angeles <br> November 19th, 2021"           
date: ""              
output:                         
  xaringan::moon_reader:         
    lib_dir: libs             
    css: ["../assets/ninpo.css",  "../assets/ninjutsu.css",   "../assets/shinobi.css", "../assets/pc_custom.css", "css/pc_custom.css"]    
    seal: true   
    self_contained: false  
    nature: 
      ratio: "16:10"
      highlightStyle: github     
      highlightLines: true 
      countIncrementalSlides: false  
---
exclude: true   
<style type="text/css">
code.r{ 
  font-size: 16px; 
}
pre {
  font-size: 16px !important;  
}
</style>
```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r run, eval=FALSE, include=FALSE}
# xaringan::inf_mr("grinnell-2021/index.rmd")
```



---

class: split-two 

.column.bg-main1[.content[

### Hi! I'm Dr. Phil Chodrow.

<br>
I look like this $\rightarrow$

Things I .alert[like]:]

- Computation and theory for complex systems
  - Network data analysis
  - Modeling social systems
  - Machine learning
- Tea
- Memes
- *Star Trek: Deep Space 9*  
- Traditional martial arts
]

.column[.content.center[.stretch[<br><br><br>
<img src="../img-shared/star-wars-trek.png" width=100%>]]


]

---

class: bg-main1 
background-image: url("../img-shared/geo-intro.png")
background-size: contain
 
# My Journey
 
---

class: split-two

.column.bg-main1[
## 45 minutes from now...

<br>

...You will be able to conceptually describe the "algorithm" in the phrase "machine learning algorithm."

...You will have proven .alert[correctness] and .alert[convergence] rates for simple optimization examples. 

...You will have discussed an instance of .alert2[algorithmic bias] arising from the formulation of an optimization problem. 

]

.column[
## The plan

<br>

1. Intro to optimization algorithms 
2. .alert[Analysis activity]: bisection search. 
3. .alert[Analysis activity]: gradient descent. 
4. Applications to machine learning. 
5. .alert2[Discussion]: optimization, machine learning, and algorithmic bias. 

]

---

class: split-two

.column.bg-main1[
### 1D Continuous Optimization

<br>

Given a function differentiable $y = f(x)$, find the .alert[*minimum*] $x_*$ where $f(x_*)$ is smallest on some interval. 

Sometimes, we can solve $f'(x) = 0$. 

<br> 

Most of the time, though, we can't. 

So, we need iterative algorithms to find the minimum.  

***Ongoing assumption***: *there is only one minimum.*

]

.column[

<img src="img/optimization-intro.png" width=100%> 

] 

---

class: split-two


.column[
## Goal for Optimization Algorithms

It's not guaranteed possible to even represent $x_*$ in machine precision, so we don't expect to find it exactly. 

We have to tolerate some error $\epsilon$. 

**Convergence Rate Analysis** 

*In order to ensure*  $|x_t - x_*| < \epsilon$, *I need to do  $t \in O(g(\epsilon))$ iterations.*  
]

.column[
<br> <br> <br> 
<img src="img/converging.png" width=100%> 

]


---

class: 

## Groups

<br> <br> <br>

1. Groups of 2-3. 
2. Check the *second* letter of your *first* name. 
3. First person alphabetically is your group's **spokesperson**. 


---

class: split-two

.column.bg-main1[
  ## .alert[Analysis Activity 1]

<br>
Here's a sketch of **bisection search**  for finding the minimum $x_*$ of a differentiable $f$ on an interval $[a, b]$. 

1. Pick a starting window. 
2. Pick an $x_t$ in the window. 
3. Determine whether $x_*$ is to the right or left of $x_t$.  
3. Update window. 
4. Repeat. 

Fill in the details, and analyze: how many iterations $t$ must we perform to ensure that $\left|x_t - x_*\right| < \epsilon$?
]

.column[

## Assumptions

<br>
You are allowed to evaluate $f$ and its derivative $f'$ at any point. 

The derivative always "points towards" the minimum: 

- $x > x_* \Leftrightarrow  f'(x) < 0$
- $x < x_* \Leftrightarrow  f'(x) > 0$

There is exactly one minimum between points $a$ and $b$.  
]

---


class: 
background-image: url("img/bisection-search.gif")
background-size: contain

### The Problem: Multiple Variables...

---

class: 

## The Idea of Gradient Descent

Use not only the *sign* of $f'(x)$, but also its *value* in order to make an update. 

$$x_{t+1} = x_t - \underbrace{s}_{\text{step size}} f'(x_t)$$ 

If $f$ is a function of multiple variables, we can do this:

$$\left(\begin{matrix}x_{t+1} \\ y_{t+1}\end{matrix}\right) = \left(\begin{matrix}x_{t} \\ y_{t}\end{matrix}\right) - s \left(\begin{matrix}\frac{\partial f(x_t, y_t)}{\partial x} \\ \frac{\partial f(x_t, y_t)}{\partial y}\end{matrix}\right)$$

That last bit is the *gradient* $\nabla f$ of $f$, and points in the direction of steepest increase/decrease. 

.alert[Imagine]: you are a hiker going down a mountain by taking small steps in the direction of steepest descent. 


---

class: split-two

.column.bg-main1[
  
## .alert[Analysis Activity 2]

<br>

Let's use gradient descent to estimate the minimum of $f(x) = x^2$. 

$$x_{t+1} = x_t - s f'(x_t)$$

The gradient (derivative) of $f$ is $f'(x) = 2x$. Use a starting point $x_0$. 

1. For what step sizes $s$ will gradient descent correctly converge to $x_*$? 
2. How many iterations $t$ must we perform to ensure that $x_t$ satisfies $\left|x_t - x_*\right| < \epsilon$? 
]

.column[

<br> <br> <br> <br>

- For item 2, use the fact that $x_* = 0$. 
- This might feel like cheating, but the result you get actually holds for a large class of functions (the *strongly convex* ones) even when we don't know $x_*$. 

  
]


---

class: 
background-image: url("img/gradient-descent.gif")
background-size: contain

---

class: 

## Training a Machine Learning Model

- Predictor data $X = (x_1,\ldots,x_n)$. 
- Target data $Y = (y_1,\ldots,y_n)$.

Want to find a model $m$ such that $m(X) \approx Y$. 

So, we often minimize a measure of difference, called the *loss function*. 

**Example**: linear regression, $m(x_i) = ax_i+b$, want to find $a$ and $b$. 

Common approach: solve the *least-squares* problem

$$\text{Loss}(a,b) =  \sum_{i = 1}^n (ax_i + b - y_i)^2\;.$$

We try to solve $\min_{a,b} \text{Loss}(a,b)$. 

---

class: 
background-image: url("img/regression.gif")
background-size: contain

---

class: 
background-image: url("img/regions.gif")
background-size: contain

---


class: 
background-image: url("img/final-regions.png")
background-size: contain

---

class: split-two

.column.bg-main1[

## .alert[Discussion]

<br>

You are going to design a system to determine whether a hospital patient is "high risk." This information will used to inform a recommendation to admit that patient to a personalized care program. 

You have access to complete medical histories and insurance claims.

You are going to solve a machine learning optimization problem: 

$$\min_m \text{loss}(m(\text{predictor data}), \text{target data})$$

]
.column[

## &nbsp;

<br> 

1. What outcome will your algorithm aim to predict? (i.e. your *target data*) 
2. What kinds of information would you include in your predictor data? 
3. What kinds of information would you specifically **not** include in your predictor data? 

]

---

class:split-two

.column[
  <br>
  <img src="img/medical-bias.png" width=90%> 
]

.column[
  <br>
  <img src="img/dissecting-bias.png" width=100%> 
]

---

class: bg-main1

# Wrapping Up 

<br> 
Optimization algorithms lie at the heart of modern machine learning and data science. 

They often admit correctness guarantees and performance analysis. 

Careful reflection on .alert[what exactly to optimize], and what data to include, is one of the primary places in which we can mitigate algorithmic bias. 

--

### Thanks y'all!

---

### Extra: Convergence of Gradient Descent In General

<br>

Suppose that $f$ is convex, differentiable, and satisfies a **Lipschitz criterion**: 

$$\lVert \nabla f(\mathbf{x}) - \nabla f(\mathbf{y}) \rVert_2 \leq L \lVert \mathbf{x} - \mathbf{y} \rVert_2\;.$$
 
Then, gradient descent with stepsize $s = \frac{1}{L}$ reaches error tolerance $\epsilon$ after $O(\epsilon^{-1})$ iterations. 

If in addition $f$ is **strongly convex** (roughly, can be sandwiched between parabolas), then we can reach the $O(- \log \epsilon)$ rate from our parabola analysis before. 